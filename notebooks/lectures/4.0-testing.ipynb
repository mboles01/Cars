{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border: 0px solid black;\">\n",
    "    <tr style=\"width: 100%; border: 0px solid black;\">\n",
    "        <td style=\"width:75%; border: 0px solid black;\">\n",
    "            <a href=\"http://www.drivendata.org\">\n",
    "                <img src=\"https://s3.amazonaws.com/drivendata.org/kif-example/img/dd.png\" />\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "# Data Science is Software\n",
    "## Developer #lifehacks for the Jupyter Data Scientist\n",
    "\n",
    "### Section 4:  Don't let other people break your toys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Many machine learning algorithms have a curious property: they are robust against bugs. Since they’re designed to deal with noisy data, they can often deal pretty well with noise caused by math mistakes as well. If you make a math mistake in your implementation, the algorithm might still make sensible-looking predictions. This is bad news, not good news. It means bugs are subtle and hard to detect. Your algorithm might work well in some situations, such as small toy datasets you use for validation, and completely fail in other situations — high dimensions, large numbers of training examples, noisy observations, etc.\" — Roger Gross, \"[Testing MCMC code, part 1: unit tests](https://hips.seas.harvard.edu/blog/2013/05/20/testing-mcmc-code-part-1-unit-tests/)\", Harvard Intelligent Probabilistic Systems group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJ_ROOT = os.path.abspath(os.path.join(os.pardir, os.pardir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `numpy.testing`\n",
    "Provides useful assertion methods for values that are numerically close and for numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.random.normal(0.0, 1.0, 1000000)\n",
    "assert np.mean(data) == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(np.mean(data), 0.0, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.random.normal(0, 0.0001, 10000)\n",
    "b = np.random.normal(0, 0.0001, 10000)\n",
    "\n",
    "np.testing.assert_array_equal(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(a, b, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [engarde]() decorators\n",
    "\n",
    "A new library that lets you practice defensive program--specifically with pandas `DataFrame` objects. It provides a set of decorators that check the return value of any function that returns a `DataFrame` and confirms that it conforms to the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import engarde.decorators as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame({'a': np.random.normal(0, 1, 100),\n",
    "                          'b': np.random.normal(0, 1, 100)})\n",
    "\n",
    "@ed.none_missing()\n",
    "def process(dataframe):\n",
    "    dataframe.loc[10, 'a'] = 1.0\n",
    "    return dataframe\n",
    "\n",
    "process(test_data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`engarde` has an awesome set of decorators:\n",
    "\n",
    " - `none_missing` - no NaNs (great for machine learning--sklearn does not care for NaNs)\n",
    " - `has_dtypes` - make sure the dtypes are what you expect\n",
    " - `verify` - runs an arbitrary function on the dataframe\n",
    " - `verify_all` - makes sure every element returns true for a given function\n",
    "\n",
    "More can be found [in the docs](http://engarde.readthedocs.org/en/latest/api.html).\n",
    "\n",
    "**`#lifehack`: test your _data science_ code. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Code coverage\n",
    "\n",
    "What are those tests getting up to? Sometimes you think you wrote test cases that cover anything that might be interesting. But, sometimes you're wrong.\n",
    "\n",
    "`coverage.py` is an _amazing_ tool for seeing what code gets executed when you run your test suite. You can run these commands to generate a code coverage report:\n",
    "\n",
    "    coverage run --source src -m pytest\n",
    "    coverage html\n",
    "    coverage report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
