{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; border: 0px solid black;\">\n",
    "    <tr style=\"width: 100%; border: 0px solid black;\">\n",
    "        <td style=\"width:75%; border: 0px solid black;\">\n",
    "            <a href=\"http://www.drivendata.org\">\n",
    "                <img src=\"https://s3.amazonaws.com/drivendata.org/kif-example/img/dd.png\" />\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "# Data Science is Software\n",
    "---------\n",
    "## Developer #lifehacks for the Jupyter Data Scientist\n",
    "\n",
    "### Section 3:  Refactoring for reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PROJ_ROOT = os.path.join(os.pardir, os.pardir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use debugging tools throughout!\n",
    "\n",
    "Don't forget all the fun debugging tools we covered while you work on these exercises. \n",
    "\n",
    " - `%debug`\n",
    " - `%pdb`\n",
    " - `import q;q.d()`\n",
    " - And (if necessary) `%prun`\n",
    "\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "You'll notice that our dataset actually has two different files, `pumps_train_values.csv` and `pumps_train_labels.csv`. We want to load both of these together in a single `DataFrame` for our exploratory analysis. Create a function that:\n",
    " - Reads both of the csvs\n",
    " - uses the `id` column as the index\n",
    " - parses dates of the `date_recorded` columns\n",
    " - joins the labels and the training set on the id\n",
    " - returns the complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_pumps_data(values_path, labels_path):\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "    \n",
    "    \n",
    "values = os.path.join(PROJ_ROOT, \"data\", \"raw\", \"pumps_train_values.csv\")\n",
    "labels = os.path.join(PROJ_ROOT, \"data\", \"raw\", \"pumps_train_labels.csv\")\n",
    "\n",
    "df = load_pumps_data(values, labels)\n",
    "assert df.shape == (59400, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "def load_pumps_data(values_path, labels_path):\n",
    "\n",
    "    train = pd.read_csv(values_path, index_col='id', parse_dates=[\"date_recorded\"])\n",
    "    labels = pd.read_csv(labels_path, index_col='id')\n",
    "\n",
    "    return train.join(labels)\n",
    "\n",
    "values = os.path.join(PROJ_ROOT, \"data\", \"raw\", \"pumps_train_values.csv\")\n",
    "labels = os.path.join(PROJ_ROOT, \"data\", \"raw\", \"pumps_train_labels.csv\")\n",
    "\n",
    "df = load_pumps_data(values, labels)\n",
    "\n",
    "assert df.shape == (59400, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Now that we've loaded our data, we want to do some pre-processing before we model. From inspection of the data, we've noticed that there are some numeric values that are probably not valid that we want to replace.\n",
    "\n",
    " - Select the relevant columns for modeling. For the purposes of this exercise, we'll select:\n",
    "        useful_columns = ['amount_tsh',\n",
    "                      'gps_height',\n",
    "                      'longitude',\n",
    "                      'latitude',\n",
    "                      'region',\n",
    "                      'population',\n",
    "                      'construction_year',\n",
    "                      'extraction_type_class',\n",
    "                      'management_group',\n",
    "                      'quality_group',\n",
    "                      'source_type',\n",
    "                      'waterpoint_type',\n",
    "                      'status_group']\n",
    "\n",
    " - Replace longitude, and population where it is 0 with mean for that region.\n",
    "       zero_is_bad_value = ['longitude', 'population']\n",
    "       \n",
    " - Replace the latitude where it is -2E-8 (a different bad value) with the mean for that region.\n",
    "       other_bad_value = ['latitude']\n",
    "      \n",
    " - Replace construction_year less than 1000 with the mean construction year.\n",
    " - Convert object type (i.e., string) variables to categoricals.\n",
    " - Convert the label column into a categorical variable\n",
    " \n",
    "\n",
    "A skeleton for this work is below where `clean_raw_data` will call `replace_value_with_grouped_mean` internally. \n",
    "\n",
    "**Copy and Paste the skeleton below into a Python file called `preprocess.py` in `src/features/`. Import and autoload the methods from that file to run tests on your changes in this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def clean_raw_data(df):\n",
    "    \"\"\" Takes a dataframe and performs four steps:\n",
    "            - Selects columns for modeling\n",
    "            - For numeric variables, replaces 0 values with mean for that region\n",
    "            - Fills invalid construction_year values with the mean construction_year\n",
    "            - Converts strings to categorical variables\n",
    "            \n",
    "        :param df: A raw dataframe that has been read into pandas\n",
    "        :returns: A dataframe with the preprocessing performed.\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "def replace_value_with_grouped_mean(df, value, column, to_groupby):\n",
    "    \"\"\" For a given numeric value (e.g., 0) in a particular column, take the\n",
    "        mean of column (excluding value) grouped by to_groupby and return that\n",
    "        column with the value replaced by that mean.\n",
    "\n",
    "        :param df: The dataframe to operate on.\n",
    "        :param value: The value in column that should be replaced.\n",
    "        :param column: The column in which replacements need to be made.\n",
    "        :param to_groupby: Groupby this variable and take the mean of column.\n",
    "                           Replace value with the group's mean.\n",
    "        :returns: The data frame with the invalid values replaced\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "# Load the \"autoreload\" extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# always reload modules marked with \"%aimport\"\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(PROJ_ROOT, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "# import my method from the source code\n",
    "%aimport features.preprocess_solution\n",
    "from features.preprocess_solution import clean_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cleaned_df = clean_raw_data(df)\n",
    "\n",
    "# verify construction year\n",
    "assert (cleaned_df.construction_year > 1000).all()\n",
    "\n",
    "# verify filled in other values\n",
    "for numeric_col in [\"population\", \"longitude\", \"latitude\"]:\n",
    "    assert (cleaned_df[numeric_col] != 0).all()\n",
    "    \n",
    "# verify the types are in the expected types\n",
    "assert (cleaned_df.dtypes\n",
    "                  .astype(str)\n",
    "                  .isin([\"int64\", \"float64\", \"category\"])).all()\n",
    "\n",
    "# check some actual values\n",
    "assert cleaned_df.latitude.mean() == -5.970642969008563\n",
    "assert cleaned_df.longitude.mean() == 35.14119354200863\n",
    "assert cleaned_df.population.mean() == 277.3070009774711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Now that we've got a feature matrix, let's train a model! Add a function as defined below to the **`src/model/train_model.py`**\n",
    "\n",
    "The function should use [`sklearn.linear_model.LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to train a logistic regression model. In a dataframe with categorical variables `pd.get_dummies` will do encoding that can be passed to `sklearn`.\n",
    "\n",
    "The `LogisticRegression` class in `sklearn` handles muticlass models automatically, so no need to use `get_dummies` on `status_group`.\n",
    "\n",
    "Finally, this method should return a [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) object that has been run with the following parameters for a logistic regression model:\n",
    "\n",
    "    params = {'C': [0.1, 1, 10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(df):\n",
    "    \"\"\" Trains a multinomial logistic regression model to predict the\n",
    "        status of a water pump given characteristics about the pump.\n",
    "    \n",
    "        :param df: The dataframe with the features and the label.\n",
    "        :returns: A trained GridSearchCV classifier\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SOLUTION\n",
    "\n",
    "#import my method from the source code\n",
    "%aimport model.train_model_solution\n",
    "from model.train_model_solution import logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = logistic(cleaned_df)\n",
    "\n",
    "assert clf.best_score_ > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just for fun, let's profile the whole stack and see what's slowest!\n",
    "%prun logistic(clean_raw_data(load_pumps_data(values, labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
